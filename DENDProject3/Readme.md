Project: Data Warehouse
------------------------

Introduction
-----------
    A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

    Task is to build ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to. You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.

Project Datasets:
-----------------
	There are two dataset provided for this project: 
		1) Song dataset
		2) Log dataset
		
Song Dataset:
--------------
        The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

Log Dataset:
------------
    The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

Schema and design:
-------------------
	A star schema approach is used in this project: Fact tables and Dimension tables are created along with staging tables. 
	Data from S3 is loaded into the staging tables and from there the FACT and DIMESION tables are loaded.
	
FACT Table : songplays  
------------------------
    songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
    Table description:records in event data associated with song plays
    Filters: records with page NextSong is loaded.

Dimension Tables
-----------------
    users - users in the app
    user_id, first_name, last_name, gender, level
    songs - songs in music database
    song_id, title, artist_id, year, duration
    artists - artists in music database
    artist_id, name, location, lattitude, longitude
    time - timestamps of records in songplays broken down into specific units
    start_time, hour, day, week, month, year, weekday

Project Steps:
-------------
    .Schemas are designed.
    .Table create and drop statments are provided in the SQL Queries.py
    .Redshift cluster is launced and appropriate secutiry group is added.
    .Deatils on database are added to the DWH.cfg
    .ETL pipelines are created in etl.py
    
Scripts used:
-------------
    There are three python scripts and one configuration file used in this project:
    
    1)sql_queries.py
    This python script contains the details on the sql queries used in the project, all drop, create and insert statments are mentioned in this script. It also contains the steps for copying the data from S3 to staging tables.
    
    2) Create_table.py
    This pythong script is used for executing the table creation and performing the data load. 
    
    3)etl.py
    The main script which invokes both the sql_queries and create_table python scripts and runs the process end to end. Dataware house tables fact and dimensions are loaded.
    
    4)dwh.cfg
    Contains the configuration details:
    
    
    5)Readme
    Contains the description of the project.
 